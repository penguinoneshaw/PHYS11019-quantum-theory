\documentclass[]{scrreprt}
\usepackage{graphicx}
\usepackage[arrowdel]{physics}
\usepackage[hidelinks]{hyperref}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{bbm}
\KOMAoption{chapterprefix}{true}

\KOMAoptions{
paper=a4,
fontsize=11pt,
parskip=half-,
BCOR=.5cm,
pagesize=auto,
pagesize=pdftex,
headinclude=false,
footinclude=false,
DIV=12
}

\renewcommand*\raggedchapter{\centering}
\RedeclareSectionCommand[beforeskip=0pt,afterskip=1\baselineskip]{chapter}
\setkomafont{chapterprefix}{\normalsize\mdseries}

\renewcommand*{\chapterformat}{%
  \chapappifchapterprefix{\nobreakspace}\thechapter\autodot%
  \IfUsePrefixLine{%
    \par\nobreak\vspace{-\parskip}\vspace{-.6\baselineskip}%
    \rule{0.9\textwidth}{.2pt}%
  }{}%
}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\integerpart{[}{]}
\usepackage{mathrsfs}
\newcommand{\fourier}[1]{\ensuremath{\mathlarger{\mathcal{F}}\!\!\left[#1 \right]}}
\newcommand{\inversefourier}[1]{\ensuremath{\mathlarger{\mathcal{F}^{-1}}\!\!\left[#1 \right]}}
\newcommand{\laplace}[1]{\ensuremath{\mathlarger{L}\!\!\left[#1 \right]}}
\DeclareMathOperator{\erfc}{erfc}

\setkomafont{disposition}{\rmfamily\scshape\bfseries}


\usepackage[largesc]{newpxtext}
\usepackage{newpxmath}
\setkomafont{descriptionlabel}{\rmfamily\bfseries}
\linespread{1.05}
\setcounter{tocdepth}{1}
\usepackage[tikz]{mdframed}
\newmdenv[frametitle=Example,roundcorner=5pt]{example}
\newmdenv[frametitle=Note,roundcorner=5pt]{note}
\renewcommand\hbar\hslash
\newcommand{\DD}[1]{\ensuremath{\mathcal{D} #1\,}}

\title{Quantum Theory}
\subtitle{Integrals. SO MANY INTEGRALS.}
\author{James Shaw}
\date{Spring 2017}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\vec}{\underline}
\setcounter{tocdepth}{0}

%\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,shrink=10]{microtype}
\usepackage[activate={true,nocompatibility},kerning=true,spacing=true,final]{microtype}

\begin{document}
\input{titlepage}

\tableofcontents % prints Table of Contents

\part{Tiny Little Things Sort Of Moving Around Not Actually That Fast\ldots{} Maybe}

\chapter{Back to Basics --- Quantum Mechanics Style}
\section{Double Slit Experiment}

The double slit experiment demonstrates some of the most important points of quantum mechanics.
Feynman was a particular fan of it. We make $P_1 = \abs{\phi_1}^2$ the probability of a particle passing through the first slit and likewise for $P_2$.
Classically, we'd expect that $$P_{1\text{ or }2} = P_1 + P_2 = \abs{\phi_1}^2 + \abs{\phi_2}^2,$$ which is all well and good but isn't what happens when we approach the quantum world.

We define two states, $\ket{i}$ and $\ket{f}$, the initial and final states, and two intermediate states $\ket{1}, \ket{2} \in \mathcal{H}$ for each of the slits.
These are vectors in a Hilbert space, and as such are linear superposable and have dual basis vectors $\bra{i}, \bra{f}, \bra{1}, \bra{2} \in \mathcal{H}^\ast$.
These define the normalisation ($\braket{\psi} = 1$) and are used to great effect elsewhere.

The probabilities in this formalism are defined by
\begin{align*}
  P(i \to f) &= \abs{\bra{f}\ket{i}}^2\\
   &= \abs{\bra{f}\ket{1}\bra{1}\ket{i} + \bra{f}\ket{2}\bra{2}\ket{i}}^2\\
  &= \abs{\bra{f}\ket{1}\bra{1}\ket{i}}^2 + \abs{\bra{f}\ket{2}\bra{2}\ket{i}} + 2 \Re(\bra{f}\ket{1}\bra{1}\ket{i}^\ast \bra{f}\ket{2}\bra{2}\ket{i})
\end{align*}
where the completeness of the Hilbert space has been used.
This asserts that \[ \hat{\mathbbm{1}} = \sum_{\text{states}\, k} \dyad{k}\] where \( \ket{k}\in\mathcal{H} \) are orthonormal basis vectors.
This is what is meant by ``inserting a complete set of states''.
Othonormality of discrete eigenvectors (eignfunctions, eigenstates\ldots) is defined by \[ \braket{n}{m} = \delta_{mn}\] and in the continuous limit is defined by \[\braket{x}{x^\prime} \delta(x-x^\prime).\]

We can change the basis of a set of states with \[\ket{\bar{n}} = \sum_{m} \ket{m}\braket{m}{\bar{n}} \] where \(\braket{m}{n} = U_{mn}\) where $U \in \mathrm{SL}(n)$.

\section{Operators and Observables}
An observable \(\hat\xi = \sum_n \xi_n \op{n}\) is an operator which satisfies the following properties:
\begin{enumerate}
  \item That there are states \(\ket{n}\) which are eigenstates of $\hat \xi$ such that $\xi_n \in \mathbb{R}$ are the eigenvalues.
  This comes from the spectral theorem and representation.
  \item It is hermitian, that is to say \[\mel{\phi}{\hat\xi^\dagger}{\psi} =\mel{\phi}{\hat\xi}{\psi}\] which requires that the eigenvalues $\xi_n$ are real by the spectral theorem.
  \item It is complex-linear, such that \[ \hat\xi(c_1 \ket{\psi_1} + c_2 \ket{\psi_2} ) = c_1\hat\xi \ket{\psi_1} + c_2\hat\xi \ket{\psi_2} \] for $c_i \in \mathbb{C}$. This comes from the definition of the Hilbert space.
  \item It commutes with other observables, such that if $\hat{\chi}$ is an observable, then $\commutator{\hat\chi}{\hat\xi}=0$.
\end{enumerate}

When we measure an observable, we get a collapse of the quantum states.
This is shown through the use of the \emph{projection} operator, \[\hat{P}_n = \ketbra{n}\] which has the effect of throwing away all other states in a composite state.
For example, \[ \hat{P}_1 (c_1\ket{1} + c_2\ket{2}) = c_1\ket{1}\braket{1} + c_2\ket{1}\bra{1}\ket{2} = c_1 \ket{1} \] which seems nice and obvious in the maths, but the physics behind it is \emph{weird}.

Degeneracy also makes this weird, but fundamentally it's just adding in more sums until everything is nicely in one state.

\section{Squish It All Together}
We move to the continuous variables of position and momentum space.
This is equivalent to having infinite slits in your diffraction grating.

\subsection{Position}
Position space is defined such that \[ \int_a^b \psi(x) \ket{x} \dd{x}\] with the orthonormality condition \[\braket{x}{x^\prime}=\delta(x-x^\prime). \]
This means we can define the projection on to position space as a function of the position, i.e.~\[\braket{x}{\psi} = \psi(x).\]
The position operator is then \[ \hat{x} = \int x \ketbra{x} \dd{x} \] which is also known as the spectral resolution, and we've used the continuum analogue of the identity operator
\[ \hat{\mathbbm{1}} = \int \ketbra{x}\dd{x}. \]

\subsection{Momentum}
We use the fourier transform to find the momentum space.
We define states \(\ket{k}\) such that \[ \ket{k} = \frac{1}{\sqrt{2\pi}} \int e^{ikx} \ket{k} \dd{x}\] which allows us to make the identifications
\begin{align*}
  \braket{x}{k} &= \frac{1}{\sqrt{2\pi}}e^{ikx} & \braket{k}{x} &= \frac{1}{\sqrt{2\pi}}e^{-ikx}
\end{align*}
which shows that it is a unitary transformation between bases.

It's worth noting that \[ \tilde\psi(k) = \mathcal{F}\qty[\psi(x)].\]

\subsection{Working Between The Two}
We use Feynman's trick to find the action of the momentum operator on the position basis, given by
\begin{align*}
  \hat{k} \ket{x} &= \frac{1}{\sqrt{2\pi}} \int k e^{-ikx} \ket{k} \dd{k}\\
  &= \frac{i}{\sqrt{2\pi}} \pdv{x} \int e^{-ikx} \ket{k} \dd{k}\\
  &= i \pdv{x} \ket{x}
\end{align*}
which gives us the final results
\begin{align*}
  \mel{x}{\hat{k}}{\psi} &= -i\pdv{x} \psi & \mel{k}{\hat{x}}{\tilde\psi} &= i\pdv{k}\tilde\psi.
\end{align*}

\subsection{Canonical Commutation Relation}
The fundamental result of quantum mechanics is given by the canonical commutation relation, \[ \commutator{\hat{x}}{\hat{k}} = i \] which more practically is used as \[\commutator{\hat{x}}{\hat{p}} = i\hbar. \]

\section{Time is a Continuum. Lunchtime doubly so}
We now increase the number of gratings, and take it to its continuum limit.
Firstly, we define a system with \(N\) gratings arranged one after another.
This gives us states \( \ket{x_i, t_i} \) where the \(i\) labels the grating, so \(t_i\) is the time the \(i\)-th grating is passed.
Using the Hilbert completeness relation, this gives us for \(N=1\) \[\braket{f}{i} = \int \braket{f}{x_1,t_1} \braket{x_1,t_1}{i}\dd{x_1}.\]
Increasing \(N\) further gives us \[\braket{f}{i} = \idotsint \braket{f}{x_1,t_1} \braket{x_1,t_1}{x_2,t_2} \bra{x_2, t_2}\cdots\ket{x_n,t_n}\braket{x_n, t_n}{f}\dd{x_1}\cdots\dd{x_n}\] where the increasing number of integrals increases the precision, but makes actual calculations difficult.

Defining \(\ket{i} = \ket{x_0, t_0} \) and \( \ket{f} = \ket{x_{N+1}, t_{N+1}} \) allows us to evenly space the gratings a distance \( \varepsilon \) from each other with
\begin{alignat*}{3}
  t_n &= t_0 + n\varepsilon & \qquad \text{where}&\qquad& \varepsilon &= \frac{t_{N+1} - t_0}{N+1}
\end{alignat*}
which then allows us to write the transition amplitude as
\begin{align*}
  \braket{x_b, t_b}{x_a, t_a} &= \qty(\prod_{n=1}^{N}\int\dd{x_n})\qty(\prod_{n=1}^{N+1}\braket{x_{n}, t_{n}}{x_{n-1}, t_{n-1}}).
\end{align*}

Finally, we define a new measure, which compactifies this notation for the \(\varepsilon \to 0 \) limit.
This is \[ \braket{x_a, t_a}{x_b, t_b} =  \eval{\int_{x_a}^{x_b}\DD{x}\braket{x_a, t_a}{x_b, t_b}}_{x(t)}. \]

\chapter{Moving All The Things Around On A Path We're Not Quite Sure Of}
\section{Classically Speaking, Actions Are Awkward}
The Lagrangian definition of the action is \[ S[x(t)] = \int_{t_a}^{t_b} L(x, \dot{x}, t) \dd{t} \]
where \(L = T-V \) is the classical Lagrangian (of a point particle in some generalised co-ordinate) and \(S[x(t)]\) is a functional of the position. We also have canonical momenta \[ p = \pdv{L}{\dot{x}} \] which coincide with regular momenta in most cases, rather like the energy function.

We define the functional derivative
\[ \eval{\frac{\delta S}{\delta x}}_{x = \bar x} = 0 \] which is a statement of Hamilton's principle, defining the classical trajectory where the endpoints are fixed. What it actually \emph{means} is that we vary the action along a path by an infinitessimal using \[ \delta S = S[x + \delta x] - S[x] \] and Taylor expand. This leads to the Euler-Lagrange equation for the classical trajectory, \[ \dv{t}(\pdv{L}{\dot x}) - \pdv{L}{x} = 0 \] which defines the classical trajectory \(\bar{x} \) on which the \emph{classical action}
\(S_{cl} = S[\bar x(t)]\) is defined. This will come back later.

\section{Here Come The Irish With Their Principle Functions}
If we don't fix the later-in-time endpoint in space, we derive Hamilton's principle function.
The variation of \( (x_b, t_b) \mapsto (x_b, t_b) + (\delta x_b, \delta t_b)\) (where we then take \(\variation{t_b} = 0\)) gives us that this variation is \begin{align*}
  \variation{S_{cl}} &= \eval{\pdv{L}{\dot{x}}\variation{x}}^{t_b}\\
  &= p(t_b)\variation{x}\\
  \implies \pdv{S_{cl}}{x} &= p_b.
\end{align*}

Doing the equivalent variation for variation-in-time for the endpoint in space, we get the Hamilton-Jacobi equation for the Hamiltonian, \[\variation{S_{cl} = - E_b \variation{t_b}}\] or otherwise written,
\[E(x_b, \pdv{S_{cl}}{x_b}, t_b) + \pdv{S_{cl}}{t_b} = 0\] which \emph{defines} Hamilton's principle function, which we've suspiciously also named \(S_{cl}\).

\section{Just how probable are these paths, then?}
We go back to the transition amplitudes.
Through some slightly iffy arguments involving ``intuition'', we define the path amplitudes as
\begin{align*}
  \eval{\braket{x_a, t_a}{x_b, t_b}}_{x(t)} &\sim \exp[i\int_{t_a}^{t_b} \!\!\!\phi(x, \dot x, t) \dd{t}]\\
  &= e^{\nicefrac{i}{\hbar} S[x(t)]}.
\end{align*}
which we can get an expression for the full transition with
\begin{align*}
  \braket{x_b, t_b}{x_a, t_a} &= \int_{x_a}^{x_b}\!\DD{x} e^{\nicefrac{i}{\hbar} S[x(t)]}.
\end{align*}

Now, this still has the funny \(\DD{x}\) thing, which is defined as
\[\int_{x_a}^{x_b}\!\DD{x} = \lim_{N\to \infty} A_N \prod_{n=1}^N \int_{-\infty}^{\infty} \dd{x_{n}} \] where \(A_N = (\nu(\varepsilon))^{N+1}\) is a normalisation factor equal across each discrete step.
Ideally, it should make the entire thing converge, but we now need to find what \(\nu(\varepsilon)\) is.


\appendix
\chapter{Gaussian Integrals}
\section{Basic Gaussians}
Lots of Gaussian integrals happen in this subject. In the interests of not getting stuck, here are the results.

The basic Gaussian integral for \( a \in \mathbb{R}^+\) is
\[ \int_\mathbb{R} e^{-ax^2} \dd{x} = \sqrt\frac{\pi}{a} \] which we can then use Feynman's trick of differentiation with respect to the parameter and symmetry arguments to find
\begin{align*}
  \int_\mathbb{R} x^n e^{-ax^2} \dd{x} &= \begin{cases}
    \pdv[\nicefrac{n}{2}]{a} \sqrt\frac{\pi}{a} & \text{if \(n\) even}\\
    0 & \text{otherwise}
  \end{cases}.
\end{align*}

If there is an \(e^{bx} \) power, this may be taken care of using completing the square, given
\begin{align*}
  \int_\mathbb{R} e^{-ax^2+bx} \dd{x} &= \int_\mathbb{R} e^{-a(x - \nicefrac{b}{2a})^2+\nicefrac{b^2}{4a}} \dd{x}\\
  &= \sqrt\frac{\pi}{a} e^{\nicefrac{b^2}{4a}}
\end{align*}
and if we make \(b = ik\) this still works, which shows that the fourier transform of a Gaussian is still a Gaussian.
With these, Feynman's trick still works, and works even better than before as we now have single \(x\) powers to play with --- just keep differentiating.

By closing one's contour cleverly, one can also solve Fresnel (and similar) integrals with a complex integrand. This gives
\begin{align*}
  \int_\mathbb{R} e^{-iax^2+ikx} \dd{x}
  &= \sqrt\frac{i\pi}{a} e^{\nicefrac{-k^2}{4a}}
\end{align*}
which is the same as what you get if you just substitute into the previous result, but it's not obvious that that should be the case.

\section{Nested Gaussians}
When it is said that there are a lot of Gaussians, what makes it all the harder is that they all happen \emph{all at once}.

Firstly, we look at a double Gaussian, in the form
\begin{align*}
  I &= \int_{-\infty}^\infty e^{\nicefrac{i}{a} {(x-u)^2}}e^{ \nicefrac{i}{b}(u-y)^2}\dd{u}
\end{align*}
which is a bit tricky because they're coupled.
We use a clever trick, by which we use the translational invariance of these integrals, making a mapping \( u \mapsto u + y \) which makes the whole thing a function of \( z = x - y \).
Utilising this gives us
\begin{align*}
  \frac{i}{a}(z-u)^2 + \frac{i}{b}u^2 &= \frac{i}{a}(z^2 - 2zu + u^2) + \frac{i}{b}u^2\\
  &= i\qty(\frac{1}{a} + \frac{1}{b})\qty(u - \frac{z}{a \qty(\frac{1}{a} + \frac{1}{b})})^2 + \frac{i}{a}z^2 + i \frac{z^2}{a^2 \qty(\frac{1}{a}  + \frac{1}{b})}\\
  &= i\qty(\frac{1}{a} + \frac{1}{b})\qty(u - \frac{z}{ia \qty(\frac{1}{a} + \frac{1}{b})})^2 + iz^2\frac{1}{a + b}
\end{align*}
which with another translation gives us the really nice answer that \[ I = e^{i\frac{z^2}{a + b}} \sqrt{\frac{iab\pi}{a+b}}.\]

The $N$-th nested Gaussian, \( I_N \), is the logical extension of this, and telescopes rather nicely.
\end{document}
